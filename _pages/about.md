---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I'm Dohwan Ko, a final-year Ph.D. student at <a href="https://mlv.kaist.ac.kr/" style="color: #900023; text-decoration: none;">**Machine Learning and Vision Lab (MLV)**</a> in Korea University, under the supervision of Prof.  <a href="https://pages.cs.wisc.edu/~hwkim/" style="color: #900023; text-decoration: none;">**Hyunwoo J. Kim**</a>. 
I received my B.S. degree in Computer Science and Engineering in Korea University at Feb. 2021.
I was fortunate to gain research experience through internships at <span style="color: blue;"><strong>Meta Reality Labs</strong></span> and <span style="color: blue;"><strong>NEC Labs America</strong></span>, where my work centered on multi-modal video understanding using large language models and foundation models.
Currently, my research interest includes:
- Multi-Modal Agent AI and Reasoning
- Video Large Language Models and Foundation Models
- Efficient Multi-Modal Video Understanding

Here is my <a href="../CV.pdf" style="color: #900023; text-decoratio.n: none;">**CV**</a>.  (last update: Oct. 2025).

<span style="color: #900023;"><strong>Iâ€™m looking for my next position in the industry starting in Spring 2026. If my profile aligns with your institution's needs, I would greatly appreciate the opportunity to connect. <br>Please feel free to reach out at ikodoh[AT]korea.ac.kr.</strong></span>

## Education
- **M.S & Ph.D** in Computer Science and Engineering at <a href="https://www.korea.edu/mbshome/mbs/en/index.do" style="color: #900023; text-decoratio.n: none;">**Korea University**</a>. \\
Mar. 2021 - Current

- **B.S.** in Computer Science and Engineering at <a href="https://www.korea.edu/mbshome/mbs/en/index.do" style="color: #900023; text-decoration.n: none;">**Korea University**</a>. \\
Mar. 2017 - Feb. 2021

## Work Experiences
- <img src="../images/logo/meta.png" alt="Meta logo" style="height: 1em;"> **Meta** \\
Menlo Park, CA, USA (Mar. 2025 - Jun. 2025)  \\
Research Scientist Intern at Meta Reality Labs. 

- <img src="../images/logo/nec.png" alt="NEC logo" style="height: 2em;"> **NEC Labs America** \\
San Jose, CA, USA (Jun. 2024 - Aug. 2024) \\
Research Scientist Intern at Media Analytics Team. 

- <img src="../images/logo/naver.png" alt="NAVER logo" style="height: 1em;"> **NAVER** \\
Seongnam, South Korea (Jul. 2020 - Aug. 2020) \\
Intern at User Feedback Platform. 

## News
<div class="news-item">
  <span class="news-date">Jun. 2025</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval (BLiM)</strong> has been accepted to ICCV 2025 as a <strong>Highlight</strong>! (top 9.7%)</span>
  <!-- <span class="news-content">Our paper <strong>Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval</strong> has been accepted to ICCV 2025 (Highlight)!</span> -->
</div>

<div class="news-item">
  <span class="news-date">Sep. 2024</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>LLaMo: Large Language Model-based Molecular Graph Assistant</strong> has been accepted to NeurIPS 2024!</span>
</div>

<div class="news-item">
  <span class="news-date">Oct. 2023</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Large Language Models are Temporal and Causal Reasoners for Video Question Answering (Flipped-VQA)</strong> has been accepted to EMNLP 2023!</span>
</div>

<div class="news-item">
  <span class="news-date">Jul. 2023</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Open-vocabulary Video Question Answering (OVQA)</strong> has been accepted to ICCV 2023!</span>
</div>

<div class="news-item">
  <span class="news-date">Feb. 2023</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Meta Loss Transformer (MELTR)</strong> has been accepted to CVPR 2023!</span>
</div>

<div class="news-item">
  <span class="news-date">Nov. 2022</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Randomly Shuffled Convolution for Self-Supervised Representation Learning (Croffle)</strong> has been accepted to Information Sciences!</span>
</div>

<div class="news-item">
  <span class="news-date">Mar. 2022</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Video-Text Representation Learning via Differentiable Weak Alignment (VT-TWINS)</strong> has been accepted to CVPR 2022!</span>
</div>

<div class="news-item">
  <span class="news-date">Oct. 2021</span>&nbsp;&nbsp;
  <span class="news-content">Our paper <strong>Search-and-Attack: Temporally Sparse Adversarial Perturbations on Videos</strong> has been accepted to IEEE Access!</span>
</div>

<style>
.news-item {
  display: flex;
  margin-bottom: 16px;
  align-items: baseline;
}

.news-date {
  min-width: 80px;
  font-weight: bold;
  color: #900023;
}

.news-content {
  flex: 1;
}
</style>