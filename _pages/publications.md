---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

## International Conference Publications
\* denotes equal contribution 
- **Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification** \\
<div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style="height:120px">
		<a class="thumbnail">
		<img src="../images/flipped_vqa.png" height="100%" alt="VidChapters-7M: Video Chapters at Scale">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>VidChapters-7M: Video Chapters at Scale</strong> <br>
	<u>Antoine Yang</u>, Arsha Nagrani, Ivan Laptev, Josef Sivic, Cordelia Schmid<br>
          NeurIPS 2023 Track on Datasets and Benchmarks <br>
          <a href="https://arxiv.org/pdf/2309.13952.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="vidchapters.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex9">bibtex</button>
	<div id="bibtex9" class="collapse">
	  <pre><tt>@inproceedings{yang2023vidchapters,
title={VidChapters-7M: Video Chapters at Scale},
author={Antoine Yang and Arsha Nagrani and Ivan Laptev and Josef Sivic and Cordelia Schmid},
booktitle={NeurIPS},
year = {2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract9">abstract</button>
	<div id="abstract9" class="collapse">
        <p style="text-align: justify;">
	        Segmenting long videos into chapters enables users to quickly navigate to the information of their interest.
            This important topic has been understudied due to the lack of publicly released datasets.
            To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total.
            VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation.
            We introduce the following three tasks based on this data.
            First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment.
            To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title.
            We benchmark both simple baselines and state-of-the-art video-language models for these three tasks.
            We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks.
            Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset.
            </p>
        </div>
          <a href="https://github.com/antoyang/VidChapters"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <span></span>
      </div>
    </div>
[[paper]](https://arxiv.org/abs/2308.11920) [code] \\
Injae Kim\*, <b>Jongha Kim</b>\*, Joonmyung Choi, Hyunwoo J. Kim\\
<span style="color:darkred">**MICCAI Workshop**</span> 2023, 1st International Workshop on Foundation Models for General Medical AI

- **Object Detection in Aerial Images with Uncertainty-Aware Graph Network** \\
[[paper]](https://arxiv.org/abs/2208.10781) [code] \\
<b>Jongha Kim</b>, Jinheon Baek, Sung Ju Hwang\\
<span style="color:darkred">**ECCV Workshop**</span> 2022, The first workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications

## Domestic Conference Publication
- **A study on the Measurement System of Apple Sugar Content Based on Smartphone Camera** \\
Sanghoon Lee, <b>Jongha Kim</b>, Hyemin Song and Hyun Kim \\
Autumn Annual Conference of IEIE (The Institute of Electronics and Information Engineers) 2019