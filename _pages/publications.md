---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

## International Conference Publications
<div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style="height:120px">
		<a class="thumbnail">
		<img src="../images/flipped_vqa.png" height="100%" alt="VidChapters-7M: Video Chapters at Scale">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Large Language Models are Temporal and Causal Reasoners for Video Question Answering</strong> <br>
	        <u>Dohwan Ko\*</u>, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, Hyunwoo J. Kim<br>
          EMNLP 2023 Main<br>
          <a href="https://arxiv.org/pdf/2309.13952.pdf"><button type="button" class="btn btn-primary btn-sm">pdf</button></a>
	        <a href="vidchapters.html"><button type="button" class="btn btn-primary btn-sm">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex9">bibtex</button>
            <div id="bibtex9" class="collapse">
              <pre><tt>@inproceedings{yang2023vidchapters,
              title={VidChapters-7M: Video Chapters at Scale},
              author={Antoine Yang and Arsha Nagrani and Ivan Laptev and Josef Sivic and Cordelia Schmid},
              booktitle={NeurIPS},
              year = {2023}}</tt></pre>
            </div>
	        <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract9">abstract</button>
	          <div id="abstract9" class="collapse">
              <p style="text-align: justify;">
	        Segmenting long videos into chapters enables users to quickly navigate to the information of their interest.
            This important topic has been understudied due to the lack of publicly released datasets.
            To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total.
            VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation.
            We introduce the following three tasks based on this data.
            First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment.
            To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title.
            We benchmark both simple baselines and state-of-the-art video-language models for these three tasks.
            We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks.
            Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset.
            </p>
        </div>
          <a href="https://github.com/antoyang/VidChapters"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <span></span>
      </div>
    </div>


<div class="row pt-2">
  <div class="col-md-1 col-12"></div>
  <div class="col-md-11 col-12">
    <h3 class="secfont text-md-left">Publications</h3>
    <br>
    <div class="row mt-2">
      <div class="col-md-1 pb-3 text-md-right text-left"></div>
      <div class="col-md-3 col-sm-11 pb-3 text-md-right text-left">
        <img class="img-fluid rounded paper-img" src="../images/flipped_vqa.png" alt="concept" >
      </div>
      <div class="col-md-7 col-sm-12 text-md-left text-center">              
        <div class="col text-left text-md-left">
          <h6 class="darkgray"><b>[C13] Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models</b></h6>
    <p class="ssmall authors">Jaewoong Lee*, Sangwon Jang*, Jaehyeong Jo, <u><b>Jaehong Yoon</b></u>, Yunji Kim, Jin-Hwa Kim, Jung-Woo Ha, Sung Ju Hwang</p>
    <span class="conf">ICCV 2023 </span>&nbsp;<b> <b class="small"></b></b>
    <h6 class="black">
      <a href="https://hello3196.github.io/TCTS_FAS/" target="_blank"><span class="badge badge-project">Project Page</span></a>
      <a href="https://arxiv.org/pdf/2304.01515.pdf" target="_blank"><span class="badge badge-pdf">Paper</span></a>
      <!-- <span class="badge badge-tbd">Code</span></a> -->
      <a type="button" class="badge badge-bibtex" href="javascript:void(0)" onclick="show('bibc13')">BibTeX</a>          
    </h6>
    <div id="bibc13"  class="popup">
      @article{lee2023text,<br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Lee, Jaewoong and Jang, Sangwon and Jo, Jaehyeong and Yoon, Jaehong and Kim, Yunji and Kim, Jin-Hwa and Ha, Jung-Woo and Hwang, Sung Ju},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;journal={International Conference on Computer Vision},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2023},<br>                
      }
    </div>                  
        </div>              
      </div>
    </div>         
    <hr />


\* denotes equal contribution 
- **Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification** \\
[[paper]](https://arxiv.org/abs/2308.11920) [code] \\
Injae Kim\*, <b>Jongha Kim</b>\*, Joonmyung Choi, Hyunwoo J. Kim\\
<span style="color:darkred">**MICCAI Workshop**</span> 2023, 1st International Workshop on Foundation Models for General Medical AI

- **Object Detection in Aerial Images with Uncertainty-Aware Graph Network** \\
[[paper]](https://arxiv.org/abs/2208.10781) [code] \\
<b>Jongha Kim</b>, Jinheon Baek, Sung Ju Hwang\\
<span style="color:darkred">**ECCV Workshop**</span> 2022, The first workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications

## Domestic Conference Publication
- **A study on the Measurement System of Apple Sugar Content Based on Smartphone Camera** \\
Sanghoon Lee, <b>Jongha Kim</b>, Hyemin Song and Hyun Kim \\
Autumn Annual Conference of IEIE (The Institute of Electronics and Information Engineers) 2019